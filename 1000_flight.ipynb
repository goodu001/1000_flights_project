{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GyTn1XZ1-3Va8yZ2upCo7AeWGLtx9KUv",
      "authorship_tag": "ABX9TyOFTyce6WLEBqqpq8kbpWE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goodu001/1000_flights_project/blob/main/1000_flight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSPOneWYvt9Y",
        "outputId": "381940f9-f0d9-4fe1-bd99-eb1c3209eefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame after initial processing:\n",
            "| FlightID   | FlightNumber   | Date_Local   | Departure_Local   | Arrival_Local   | Date_UTC   | Departure_UTC   | Arrival_UTC   | Origin   | Destination   | Aircraft   | ULD_Details   | Total_ULDs   | Status    |\n",
            "|:-----------|:---------------|:-------------|:------------------|:----------------|:-----------|:----------------|:--------------|:---------|:--------------|:-----------|:--------------|:-------------|:----------|\n",
            "| 1000       | RG559          | 2025-01-18   | 7:30              | 15:15           | 2025-01-18 | 1:30            | 8:15          | SIN      | DXB           | B737       | AKE×17,P1P×12 | 29           | Cancelled |\n",
            "| 1001       | RG891          | 2025-06-16   | 8:15              | 11:15           | 2025-06-16 | 1:15            | 2:15          | SYD      | LAX           | B787       | RKN×9,9       | Cancelled    |           |\n",
            "| 1002       | RG967          | 2025-06-06   | 9:45              | 14:45           | 2025-06-06 | 2:45            | 8:45          | SIN      | LHR           | A380       | AKE×12,RKN×3  | 15           | Delayed   |\n",
            "| 1003       | RG611          | 2025-03-16   | 9:15              | 22:30           | 2025-03-16 | 1:15            | 15:30         | HND      | JFK           | A350       | PMC×1,1       | Arrived      |           |\n",
            "| 1005       | RG212          | 2025-01-19   | 14:45             | 3:15            | 2025-01-19 | 7:45            | 20:15         | LHR      | LAX           | B787       | AKE×17,17     | Departed     |           |\n",
            "\n",
            "Information about the DataFrame after initial processing:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 661 entries, 0 to 660\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   FlightID         661 non-null    object\n",
            " 1   FlightNumber     661 non-null    object\n",
            " 2   Date_Local       661 non-null    object\n",
            " 3   Departure_Local  661 non-null    object\n",
            " 4   Arrival_Local    661 non-null    object\n",
            " 5   Date_UTC         661 non-null    object\n",
            " 6   Departure_UTC    661 non-null    object\n",
            " 7   Arrival_UTC      661 non-null    object\n",
            " 8   Origin           661 non-null    object\n",
            " 9   Destination      661 non-null    object\n",
            " 10  Aircraft         661 non-null    object\n",
            " 11  ULD_Details      661 non-null    object\n",
            " 12  Total_ULDs       661 non-null    object\n",
            " 13  Status           329 non-null    object\n",
            "dtypes: object(14)\n",
            "memory usage: 72.4+ KB\n",
            "None\n",
            "\n",
            "Shape of DataFrame after dropping NaN 'Total_ULDs' rows: (329, 14)\n",
            "\n",
            "Information about the DataFrame after full preprocessing:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 329 entries, 0 to 658\n",
            "Columns: 180 entries, Total_ULDs to Status_Scheduled\n",
            "dtypes: bool(168), float64(3), int32(4), int64(5)\n",
            "memory usage: 82.2 KB\n",
            "None\n",
            "\n",
            "First 5 rows of the DataFrame after full preprocessing:\n",
            "| Total_ULDs   | Flight_Duration_Local_Hours   | Flight_Duration_UTC_Hours   | Departure_Local_DayOfWeek   | Departure_Local_Month   | Departure_Local_Hour   | Departure_Local_Minute   | ULD_AKE_Count   | ULD_RKN_Count   | ULD_PMC_Count   | ULD_AMJ_Count   | ULD_P1P_Count   | FlightNumber_RG106   | FlightNumber_RG111   | FlightNumber_RG125   | FlightNumber_RG127   | FlightNumber_RG130   | FlightNumber_RG132   | FlightNumber_RG144   | FlightNumber_RG146   | FlightNumber_RG148   | FlightNumber_RG157   | FlightNumber_RG165   | FlightNumber_RG167   | FlightNumber_RG171   | FlightNumber_RG173   | FlightNumber_RG180   | FlightNumber_RG181   | FlightNumber_RG189   | FlightNumber_RG193   | FlightNumber_RG194   | FlightNumber_RG195   | FlightNumber_RG199   | FlightNumber_RG203   | FlightNumber_RG204   | FlightNumber_RG212   | FlightNumber_RG214   | FlightNumber_RG227   | FlightNumber_RG241   | FlightNumber_RG242   | FlightNumber_RG246   | FlightNumber_RG256   | FlightNumber_RG259   | FlightNumber_RG261   | FlightNumber_RG263   | FlightNumber_RG266   | FlightNumber_RG267   | FlightNumber_RG296   | FlightNumber_RG303   | FlightNumber_RG314   | FlightNumber_RG317   | FlightNumber_RG320   | FlightNumber_RG324   | FlightNumber_RG325   | FlightNumber_RG328   | FlightNumber_RG334   | FlightNumber_RG338   | FlightNumber_RG350   | FlightNumber_RG352   | FlightNumber_RG357   | FlightNumber_RG373   | FlightNumber_RG374   | FlightNumber_RG376   | FlightNumber_RG381   | FlightNumber_RG384   | FlightNumber_RG400   | FlightNumber_RG432   | FlightNumber_RG444   | FlightNumber_RG448   | FlightNumber_RG452   | FlightNumber_RG463   | FlightNumber_RG470   | FlightNumber_RG479   | FlightNumber_RG487   | FlightNumber_RG488   | FlightNumber_RG489   | FlightNumber_RG505   | FlightNumber_RG508   | FlightNumber_RG510   | FlightNumber_RG529   | FlightNumber_RG532   | FlightNumber_RG538   | FlightNumber_RG545   | FlightNumber_RG559   | FlightNumber_RG564   | FlightNumber_RG573   | FlightNumber_RG579   | FlightNumber_RG605   | FlightNumber_RG611   | FlightNumber_RG617   | FlightNumber_RG646   | FlightNumber_RG649   | FlightNumber_RG658   | FlightNumber_RG665   | FlightNumber_RG670   | FlightNumber_RG674   | FlightNumber_RG680   | FlightNumber_RG691   | FlightNumber_RG697   | FlightNumber_RG698   | FlightNumber_RG703   | FlightNumber_RG710   | FlightNumber_RG716   | FlightNumber_RG718   | FlightNumber_RG723   | FlightNumber_RG733   | FlightNumber_RG742   | FlightNumber_RG743   | FlightNumber_RG750   | FlightNumber_RG754   | FlightNumber_RG756   | FlightNumber_RG765   | FlightNumber_RG771   | FlightNumber_RG777   | FlightNumber_RG786   | FlightNumber_RG792   | FlightNumber_RG796   | FlightNumber_RG798   | FlightNumber_RG799   | FlightNumber_RG801   | FlightNumber_RG814   | FlightNumber_RG818   | FlightNumber_RG821   | FlightNumber_RG833   | FlightNumber_RG835   | FlightNumber_RG838   | FlightNumber_RG839   | FlightNumber_RG846   | FlightNumber_RG847   | FlightNumber_RG858   | FlightNumber_RG859   | FlightNumber_RG862   | FlightNumber_RG864   | FlightNumber_RG868   | FlightNumber_RG873   | FlightNumber_RG877   | FlightNumber_RG881   | FlightNumber_RG886   | FlightNumber_RG887   | FlightNumber_RG891   | FlightNumber_RG894   | FlightNumber_RG924   | FlightNumber_RG925   | FlightNumber_RG926   | FlightNumber_RG928   | FlightNumber_RG954   | FlightNumber_RG963   | FlightNumber_RG967   | FlightNumber_RG975   | FlightNumber_RG981   | FlightNumber_RG982   | FlightNumber_RG996   | FlightNumber_RG997   | Origin_CDG   | Origin_DXB   | Origin_GRU   | Origin_HND   | Origin_JFK   | Origin_LAX   | Origin_LHR   | Origin_SIN   | Origin_SYD   | Destination_CDG   | Destination_DXB   | Destination_GRU   | Destination_HND   | Destination_JFK   | Destination_LAX   | Destination_LHR   | Destination_SIN   | Destination_SYD   | Aircraft_A350   | Aircraft_A380   | Aircraft_B737   | Aircraft_B777-300ER   | Aircraft_B787   | Status_Cancelled   | Status_Delayed   | Status_Departed   | Status_Scheduled   |\n",
            "|:-------------|:------------------------------|:----------------------------|:----------------------------|:------------------------|:-----------------------|:-------------------------|:----------------|:----------------|:----------------|:----------------|:----------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:---------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|:------------------|:------------------|:------------------|:------------------|:------------------|:------------------|:------------------|:------------------|:------------------|:----------------|:----------------|:----------------|:----------------------|:----------------|:-------------------|:-----------------|:------------------|:-------------------|\n",
            "| 29           | 7.75                          | 6.75                        | 5                           | 1                       | 7                      | 30                       | 17              | 0               | 0               | 0               | 12              | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | True                 | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False        | False        | False        | False        | False        | False        | False        | True         | False        | False             | True              | False             | False             | False             | False             | False             | False             | False             | False           | False           | True            | False                 | False           | True               | False            | False             | False              |\n",
            "| 15           | 5                             | 6                           | 4                           | 6                       | 9                      | 45                       | 12              | 3               | 0               | 0               | 0               | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | True                 | False                | False                | False                | False                | False                | False        | False        | False        | False        | False        | False        | False        | True         | False        | False             | False             | False             | False             | False             | False             | True              | False             | False             | False           | True            | False           | False                 | False           | False              | True             | False             | False              |\n",
            "| 19           | 11.25                         | -12.75                      | 1                           | 4                       | 6                      | 30                       | 8               | 0               | 0               | 11              | 0               | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | True                 | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False        | True         | False        | False        | False        | False        | False        | False        | False        | False             | False             | False             | False             | False             | True              | False             | False             | False             | True            | False           | False           | False                 | False           | True               | False            | False             | False              |\n",
            "| 31           | -14.5                         | 12.5                        | 0                           | 2                       | 15                     | 45                       | 0               | 0               | 0               | 12              | 19              | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | True                 | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False        | False        | False        | True         | False        | False        | False        | False        | False        | False             | False             | False             | False             | False             | True              | False             | False             | False             | False           | False           | False           | False                 | True            | False              | False            | False             | True               |\n",
            "| 15           | 6                             | 3                           | 4                           | 3                       | 15                     | 45                       | 0               | 13              | 2               | 0               | 0               | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | True                 | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False                | False        | False        | False        | False        | False        | False        | True         | False        | False        | True              | False             | False             | False             | False             | False             | False             | False             | False             | False           | True            | False           | False                 | False           | False              | False            | False             | True               |\n",
            "\n",
            "Training data shape: (263, 179)\n",
            "Testing data shape: (66, 179)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1353141671.py:117: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Status'].fillna('Unknown', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation Results ---\n",
            "Mean Absolute Error (MAE): 3.35\n",
            "Mean Squared Error (MSE): 16.19\n",
            "Root Mean Squared Error (RMSE): 4.02\n",
            "R-squared (R2): 0.73\n",
            "\n",
            "--- Project Summary ---\n",
            "This project successfully built a RandomForestRegressor model to predict 'Total_ULDs' based on various flight details.\n",
            "The model achieved an R-squared of 0.73, indicating a reasonable fit to the data.\n",
            "On average, the predictions are off by approximately 3.35 units from the actual 'Total_ULDs' values.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "path = \"/content/drive/MyDrive/flight 1000.csv\"\n",
        "# --- 1. Data Loading and Initial Inspection ---\n",
        "\n",
        "# Define the expected 15 column names after the ULD_Details split\n",
        "column_names = [\n",
        "    'FlightID', 'FlightNumber', 'Date_Local', 'Departure_Local', 'Arrival_Local',\n",
        "    'Date_UTC', 'Departure_UTC', 'Arrival_UTC', 'Origin', 'Destination',\n",
        "    'Aircraft', 'ULD_Details_Part1', 'ULD_Details_Part2', 'Total_ULDs', 'Status'\n",
        "]\n",
        "\n",
        "# Load the dataset without a header, and using the defined column names\n",
        "# Using 'engine=python' and 'on_bad_lines=skip' for robustness against parsing errors\n",
        "df = pd.read_csv(path, header=None, names=column_names, on_bad_lines='skip', engine='python')\n",
        "\n",
        "# The first row of the data is actually the header, so drop it and reset index\n",
        "df = df.iloc[1:].copy()\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Combine ULD_Details_Part1 and ULD_Details_Part2 into a single ULD_Details column\n",
        "df['ULD_Details'] = df['ULD_Details_Part1'].astype(str) + ',' + df['ULD_Details_Part2'].astype(str)\n",
        "\n",
        "# Drop the individual ULD_Details_Part columns\n",
        "df.drop(columns=['ULD_Details_Part1', 'ULD_Details_Part2'], inplace=True)\n",
        "\n",
        "# Reorder columns to have ULD_Details in its original position\n",
        "original_cols = [\n",
        "    'FlightID', 'FlightNumber', 'Date_Local', 'Departure_Local', 'Arrival_Local',\n",
        "    'Date_UTC', 'Departure_UTC', 'Arrival_UTC', 'Origin', 'Destination',\n",
        "    'Aircraft', 'ULD_Details', 'Total_ULDs', 'Status'\n",
        "]\n",
        "df = df[original_cols]\n",
        "\n",
        "print(\"First 5 rows of the DataFrame after initial processing:\")\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "print(\"\\nInformation about the DataFrame after initial processing:\")\n",
        "print(df.info())\n",
        "\n",
        "# --- 2. Data Preprocessing ---\n",
        "\n",
        "# Convert 'Total_ULDs' to numeric, coercing errors to NaN\n",
        "df['Total_ULDs'] = pd.to_numeric(df['Total_ULDs'], errors='coerce')\n",
        "\n",
        "# Drop rows where 'Total_ULDs' is NaN, as it is our target variable\n",
        "df.dropna(subset=['Total_ULDs'], inplace=True)\n",
        "print(\"\\nShape of DataFrame after dropping NaN 'Total_ULDs' rows:\", df.shape)\n",
        "\n",
        "\n",
        "# Convert date and time columns to datetime objects and engineer features\n",
        "# Function to safely combine date and time, handling potential errors\n",
        "def combine_date_time(date_col, time_col):\n",
        "    return pd.to_datetime(df[date_col] + ' ' + df[time_col], errors='coerce')\n",
        "\n",
        "df['Departure_Local_DT'] = combine_date_time('Date_Local', 'Departure_Local')\n",
        "df['Arrival_Local_DT'] = combine_date_time('Date_Local', 'Arrival_Local')\n",
        "df['Departure_UTC_DT'] = combine_date_time('Date_UTC', 'Departure_UTC')\n",
        "df['Arrival_UTC_DT'] = combine_date_time('Date_UTC', 'Arrival_UTC')\n",
        "\n",
        "# Calculate flight durations in hours\n",
        "df['Flight_Duration_Local_Hours'] = (df['Arrival_Local_DT'] - df['Departure_Local_DT']).dt.total_seconds() / 3600\n",
        "df['Flight_Duration_UTC_Hours'] = (df['Arrival_UTC_DT'] - df['Departure_UTC_DT']).dt.total_seconds() / 3600\n",
        "\n",
        "# Extract time-based features\n",
        "df['Departure_Local_DayOfWeek'] = df['Departure_Local_DT'].dt.dayofweek\n",
        "df['Departure_Local_Month'] = df['Departure_Local_DT'].dt.month\n",
        "df['Departure_Local_Hour'] = df['Departure_Local_DT'].dt.hour\n",
        "df['Departure_Local_Minute'] = df['Departure_Local_DT'].dt.minute\n",
        "\n",
        "# Drop original date and time columns and the intermediate datetime objects\n",
        "df.drop(columns=['Date_Local', 'Departure_Local', 'Arrival_Local',\n",
        "                 'Date_UTC', 'Departure_UTC', 'Arrival_UTC',\n",
        "                 'Departure_Local_DT', 'Arrival_Local_DT',\n",
        "                 'Departure_UTC_DT', 'Arrival_UTC_DT'], inplace=True)\n",
        "\n",
        "# Drop FlightID as it's an identifier and not directly predictive\n",
        "df.drop(columns=['FlightID'], inplace=True)\n",
        "\n",
        "# Parse ULD_Details to extract individual ULD types and their counts\n",
        "def parse_uld_details(uld_string):\n",
        "    if pd.isna(uld_string):\n",
        "        return {}\n",
        "    uld_types = uld_string.split(',')\n",
        "    uld_counts = {}\n",
        "    for item in uld_types:\n",
        "        if '×' in item:\n",
        "            try:\n",
        "                uld_type, count = item.split('×')\n",
        "                uld_counts[uld_type.strip()] = int(count)\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return uld_counts\n",
        "\n",
        "df['Parsed_ULD_Details'] = df['ULD_Details'].apply(parse_uld_details)\n",
        "\n",
        "# Create new features for each ULD type\n",
        "all_uld_types = set()\n",
        "for detail_dict in df['Parsed_ULD_Details']:\n",
        "    all_uld_types.update(detail_dict.keys())\n",
        "\n",
        "for uld_type in all_uld_types:\n",
        "    df[f'ULD_{uld_type}_Count'] = df['Parsed_ULD_Details'].apply(lambda x: x.get(uld_type, 0))\n",
        "\n",
        "# Drop the original 'ULD_Details' and the intermediate 'Parsed_ULD_Details'\n",
        "df.drop(columns=['ULD_Details', 'Parsed_ULD_Details'], inplace=True)\n",
        "\n",
        "# Identify categorical columns for one-hot encoding\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "# Handle missing values in Status column: fill with 'Unknown'\n",
        "if 'Status' in categorical_cols:\n",
        "    df['Status'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Apply One-Hot Encoding to categorical columns\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Impute any remaining NaN values in numerical features (e.g., from duration calculation issues)\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "print(\"\\nInformation about the DataFrame after full preprocessing:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst 5 rows of the DataFrame after full preprocessing:\")\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "# --- 3. Model Training ---\n",
        "\n",
        "# Define target variable (y) and features (X)\n",
        "y = df['Total_ULDs']\n",
        "X = df.drop(columns=['Total_ULDs'])\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# Initialize and train the RandomForestRegressor model\n",
        "# n_estimators: number of trees in the forest\n",
        "# random_state: for reproducibility\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 4. Model Evaluation ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n--- Model Evaluation Results ---\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-squared (R2): {r2:.2f}\")\n",
        "\n",
        "print(\"\\n--- Project Summary ---\")\n",
        "print(\"This project successfully built a RandomForestRegressor model to predict 'Total_ULDs' based on various flight details.\")\n",
        "print(f\"The model achieved an R-squared of {r2:.2f}, indicating a reasonable fit to the data.\")\n",
        "print(f\"On average, the predictions are off by approximately {mae:.2f} units from the actual 'Total_ULDs' values.\")"
      ]
    }
  ]
}